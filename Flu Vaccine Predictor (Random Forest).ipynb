{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import sklearn, scipy\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "sub_format = pd.read_csv(\"###.csv\")\n",
    "training_features = pd.read_csv(\"###.csv\"\", index_col=\"respondent_id\")\n",
    "training_labels = pd.read_csv(\"###.csv\", index_col=\"respondent_id\")\n",
    "test_features = pd.read_csv(\"###.csv\", index_col=\"respondent_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_copy = training_features.copy().drop(columns = [\"hhs_geo_region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy = test_features.copy().drop(columns = [\"hhs_geo_region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['h1n1_concern', 'h1n1_knowledge', 'behavioral_antiviral_meds',\n",
       "       'behavioral_avoidance', 'behavioral_face_mask', 'behavioral_wash_hands',\n",
       "       'behavioral_large_gatherings', 'behavioral_outside_home',\n",
       "       'behavioral_touch_face', 'doctor_recc_h1n1', 'doctor_recc_seasonal',\n",
       "       'chronic_med_condition', 'child_under_6_months', 'health_worker',\n",
       "       'health_insurance', 'opinion_h1n1_vacc_effective', 'opinion_h1n1_risk',\n",
       "       'opinion_h1n1_sick_from_vacc', 'opinion_seas_vacc_effective',\n",
       "       'opinion_seas_risk', 'opinion_seas_sick_from_vacc', 'age_group',\n",
       "       'education', 'race', 'sex', 'income_poverty', 'marital_status',\n",
       "       'rent_or_own', 'employment_status', 'census_msa', 'household_adults',\n",
       "       'household_children', 'employment_industry', 'employment_occupation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 775,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_copy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_field(df, field_name, mapping):\n",
    "    \"\"\"\n",
    "    Maps dataframe column values in field to those in a new dictionary\n",
    "    field_name = string\n",
    "    mapping = dictionary\n",
    "    \"\"\"\n",
    "    df[field_name] =  df[field_name].replace(mapping)\n",
    "    \n",
    "    \n",
    "def concat_fields(df, old_fields, new_field, fillna = \"MissingData\"):\n",
    "    \"\"\"\n",
    "    joins multiple string-type dataframe columns into a single field. \n",
    "    old_fields = list of strings\n",
    "    new_fields = string\n",
    "    \"\"\" \n",
    "    df[new_field] = df[old_fields].fillna(\"missing\").apply(lambda col: \" \".join(col), axis=1)\n",
    "    df.drop(columns = old_fields, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "edu_map = dict(zip(['< 12 Years', '12 Years', 'Some College', 'College Graduate'], \n",
    "                   [\"Dropout\", \"HS\", \"College\", \"Graduate\"]))\n",
    "\n",
    "updates = {\n",
    "    \"marital_status\": {\"Not Married\": \"Single\"},\n",
    "    \"race\": {\"Other or Multiple\": \"Other\"}, \n",
    "    \"education\": edu_map\n",
    "          }\n",
    "\"\"\"\n",
    "for field, mapping in updates.items():\n",
    "    remap_field(tf_copy, field, mapping)\n",
    "\n",
    "concat_fields(tf_copy, old_fields = updates.keys(), new_field = \"demographic\") \"\"\"\n",
    "\n",
    "for df in [tf_copy, test_copy]:\n",
    "    for field, mapping in updates.items():\n",
    "        remap_field(df, field, mapping)\n",
    "    concat_fields(df, old_fields = updates.keys(), new_field = \"demographic\") \n",
    "    df[\"age_group\"] = df[\"age_group\"].str[:2].astype(\"int64\")\n",
    "    df[\"employment_status\"] = tf_copy[\"employment_status\"]\n",
    "    income_map = {\"<= $75,000, Above Poverty\": 70, \"Below Poverty\": 20, \"> $75,000\": 100}\n",
    "    df[\"income_poverty\"] = df.income_poverty.replace(income_map);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf_copy\n",
    "y = training_labels\n",
    "\n",
    "text_cols = [col for col in X.columns if X[col].dtypes not in ([\"float64\", \"int64\"])]\n",
    "num_cols = [col for col in X.columns if X[col].dtypes in ([\"float64\", \"int64\"])]\n",
    "\n",
    "def combine_text_columns(df):#, to_drop=num_cols):#NUMERIC_COLUMNS + LABELS):\n",
    "    \"\"\" converts all text in each row of data_frame to single vector \"\"\"\n",
    "    \n",
    "    # Drop non-text columns that are in the df\n",
    "    #to_drop = set(to_drop) & set(df.columns.tolist())\n",
    "    #text_data = df.drop(to_drop, axis=1)\n",
    "    \n",
    "    text_cols = [col for col in X.columns if X[col].dtypes not in ([\"float64\", \"int64\"])]\n",
    "    text_data = df[text_cols]\n",
    "    \n",
    "    # Replace nans with blanks\n",
    "    #text_data.fillna(\"Missing\")#, inplace=True)\n",
    "    # Join all text items in a row that have a space in between\n",
    "    return text_data.fillna(\"MissingData\").apply(lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(combine_text_columns, validate = False)#lambda df: df[text_cols], validate=False)\n",
    "get_num_data = FunctionTransformer(lambda df: df[num_cols], validate=False)\n",
    "to_dense = FunctionTransformer(lambda sparse: sparse.toarray())\n",
    "\n",
    "#Aggregate text data and vectorize text information with a hashing function \n",
    "#(words will map to a dictionary of ints, saving computation time)\n",
    "text_pipeline = Pipeline([\n",
    "    (\"selector\", get_text_data),\n",
    "    #(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value=\"Missing\")), combine text_columns makes imputer not work\n",
    "    (\"vectorizer\", HashingVectorizer(ngram_range = (1, 3)))\n",
    "])\n",
    "\n",
    "#Aggregate numeric data and impute missing value with column mean\n",
    "num_pipeline = Pipeline([\n",
    "    #no need for selector if using ColumnTransformer instead of FeatureUnion\n",
    "    (\"selector\", get_num_data),\n",
    "    (\"imputer\", SimpleImputer(strategy = \"mean\")),\n",
    "    (\"scaler\", StandardScaler()) #added line for run 2\n",
    "     ])\n",
    "\n",
    "pl = Pipeline([\n",
    "    (\"union\", FeatureUnion([\n",
    "        (\"numeric\", num_pipeline), \n",
    "        (\"text\", text_pipeline)])),\n",
    "    (\"clf\", OneVsRestClassifier(LogisticRegression()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric-only, n = 5 has an accuarcy of: 0.8393859977536503 and an AUC_ROC of 0.6509132452748754\n",
      "Numeric-only, n = 6 has an accuarcy of: 0.8461250467989517 and an AUC_ROC of 0.6731689539518405\n",
      "Numeric-only, n = 7 has an accuarcy of: 0.8489329839011606 and an AUC_ROC of 0.683466246088957\n",
      "Numeric-only, n = 8 has an accuarcy of: 0.8538000748783228 and an AUC_ROC of 0.697911788551903\n",
      "Numeric-only, n = 9 has an accuarcy of: 0.8539872706851367 and an AUC_ROC of 0.7043331237493109\n",
      "Numeric-only, n = 10 has an accuarcy of: 0.855672032946462 and an AUC_ROC of 0.7098182755194461\n",
      "Numeric-only, n = 11 has an accuarcy of: 0.8566080119805316 and an AUC_ROC of 0.7110449950568438\n",
      "Numeric-only, n = 12 has an accuarcy of: 0.8577311868214152 and an AUC_ROC of 0.7161722370202501\n",
      "Numeric-only, n = 13 has an accuarcy of: 0.8554848371396481 and an AUC_ROC of 0.7137952930040605\n",
      "Numeric-only, n = 14 has an accuarcy of: 0.8575439910146013 and an AUC_ROC of 0.718258644926315\n",
      "Numeric-only, n = 15 has an accuarcy of: 0.8560464245600898 and an AUC_ROC of 0.7185646251607382\n",
      "Numeric-only, n = 16 has an accuarcy of: 0.8564208161737177 and an AUC_ROC of 0.7188032316985572\n",
      "Numeric-only, n = 17 has an accuarcy of: 0.8562336203669038 and an AUC_ROC of 0.7167933188510982\n",
      "Numeric-only, n = 18 has an accuarcy of: 0.8532384874578809 and an AUC_ROC of 0.7167750761270947\n",
      "Numeric-only, n = 19 has an accuarcy of: 0.8545488581055785 and an AUC_ROC of 0.7166648942201869\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.compose import make_column_selector\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[\"h1n1_vaccine\"], test_size = .2, random_state = 420)#, stratify = y[\"seasonal_vaccine\"])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric', num_pipeline, num_cols)\n",
    "    #,('text', text_pipeline, text_cols)# selector(dtype_include=\"object\"))\n",
    "])\n",
    "\n",
    "#older version of pipeline, saved for archival\n",
    "for i in range(5, 20):\n",
    "    pl_preprocessing = Pipeline([\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(max_depth = i, n_estimators = 200, random_state = 420))\n",
    "        ])\n",
    "\n",
    "    model = pl_preprocessing.fit(X_train, y_train)\n",
    "    model_predictions = model.predict(X_test)\n",
    "    model_roc = roc_auc_score(y_test, model_predictions)\n",
    "    print(f\"Numeric-only, n = {i} has an accuarcy of: {model.score(X_test, y_test)} and an AUC_ROC of {model_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric-only, n = 5 has an accuarcy of: 0.7721827031074504 and an AUC_ROC of 0.7694019753487487\n",
      "Numeric-only, n = 6 has an accuarcy of: 0.7751778360164733 and an AUC_ROC of 0.7719523770849619\n",
      "Numeric-only, n = 7 has an accuarcy of: 0.7802321228004493 and an AUC_ROC of 0.7772244951366398\n",
      "Numeric-only, n = 8 has an accuarcy of: 0.7817296892549607 and an AUC_ROC of 0.7790199847269175\n",
      "Numeric-only, n = 9 has an accuarcy of: 0.7813552976413328 and an AUC_ROC of 0.778643374651872\n",
      "Numeric-only, n = 10 has an accuarcy of: 0.7862223886184949 and an AUC_ROC of 0.7832502563373704\n",
      "Numeric-only, n = 11 has an accuarcy of: 0.7847248221639835 and an AUC_ROC of 0.7818305308242163\n",
      "Numeric-only, n = 12 has an accuarcy of: 0.7845376263571696 and an AUC_ROC of 0.7818012028962458\n",
      "Numeric-only, n = 13 has an accuarcy of: 0.7819168850617746 and an AUC_ROC of 0.7790782175838977\n",
      "Numeric-only, n = 14 has an accuarcy of: 0.7821040808685885 and an AUC_ROC of 0.779627834234039\n",
      "Numeric-only, n = 15 has an accuarcy of: 0.7837888431299139 and an AUC_ROC of 0.7812503172492208\n",
      "Numeric-only, n = 16 has an accuarcy of: 0.7802321228004493 and an AUC_ROC of 0.7776869740007918\n",
      "Numeric-only, n = 17 has an accuarcy of: 0.7789217521527517 and an AUC_ROC of 0.7762098616285799\n",
      "Numeric-only, n = 18 has an accuarcy of: 0.7779857731186821 and an AUC_ROC of 0.7755718381955653\n",
      "Numeric-only, n = 19 has an accuarcy of: 0.7776113815050543 and an AUC_ROC of 0.7748483689724057\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.compose import make_column_selector\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[\"seasonal_vaccine\"], test_size = .2, random_state = 420)#, stratify = y[\"seasonal_vaccine\"])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric', num_pipeline, num_cols)\n",
    "    #,('text', text_pipeline, text_cols)# selector(dtype_include=\"object\"))\n",
    "])\n",
    "\n",
    "#older version of pipeline, saved for archival\n",
    "for i in range(5, 20):\n",
    "    pl_preprocessing = Pipeline([\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(max_depth = i, n_estimators = 200, random_state = 420))\n",
    "        ])\n",
    "\n",
    "    model = pl_preprocessing.fit(X_train, y_train)\n",
    "    model_predictions = model.predict(X_test)\n",
    "    model_roc = roc_auc_score(y_test, model_predictions)\n",
    "    print(f\"Numeric-only, n = {i} has an accuarcy of: {model.score(X_test, y_test)} and an AUC_ROC of {model_roc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_preprocessing = Pipeline([\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(max_depth = 12, n_estimators = 200, random_state = 420))\n",
    "        ])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[\"h1n1_vaccine\"], test_size = .2, random_state = 420)#, stratify = y[\"seasonal_vaccine\"])\n",
    "\n",
    "model = pl_preprocessing.fit(X_train, y_train)\n",
    "h1n1_predictions = model.predict_proba(test_copy)\n",
    "\n",
    "pl_preprocessing = Pipeline([\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"clf\", RandomForestClassifier(max_depth = 10, n_estimators = 200, random_state = 420))\n",
    "        ])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y[\"seasonal_vaccine\"], test_size = .2, random_state = 420)#, stratify = y[\"seasonal_vaccine\"])\n",
    "model = pl_preprocessing.fit(X_train, y_train)\n",
    "seasonal_predictions = model.predict_proba(test_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"h1n1_vaccine\"] = h1n1_predictions[:,1]\n",
    "submission[\"seasonal_vaccine\"] = seasonal_predictions[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[[\"h1n1_vaccine\", \"seasonal_vaccine\"]].to_csv(\"Flu Shot Entry 12-6-20v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission Results: \n",
    "\n",
    "AUC: ~81% \n",
    "\n",
    "Top 11.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
